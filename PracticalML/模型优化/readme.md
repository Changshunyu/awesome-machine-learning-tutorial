本项目将围绕一个**表情识别项⽬**开展。

+ [1. 数据使用](#数据使用)
	+ [1.1 搜集与整理数据](#搜集与整理数据)
	+ [1.2 分析数据质量](#分析数据质量)
	+ [1.3 选择好数据的尺度](#选择好数据的尺度)
	+ [1.4 图像分类基本任务中的数据增强实战](#图像分类基本任务中的数据增强实战)
+ [2. 模型使用和调参](#模型使用和调参)
	+ [2.1 如何针对自己的任务选择好基准模型架构](#如何针对自己的任务选择好基准模型架构)
	+ [2.2 如何设计与改进模型](#如何设计与改进模型)
	+ [2.3 如何对模型进行训练调参](#如何对模型进行训练调参)
+ [3. 模型分析](#模型分析)
	+ [3.1 如何对模型进行可视化](#如何对模型进行可视化)
	+ [3.2 如何对模型的计算量和参数量进行分析](#如何对模型的计算量和参数量进行分析)
+ [4. 紧凑模型设计](#紧凑模型设计)
	+ [4.1 如何压缩大模型](#如何压缩大模型)
	+ [4.2 如何设计小模型](#如何设计小模型)
	+ [4.3 如何保障小模型的性能](#如何保障小模型的性能)
+ [5. 模型剪枝](#模型剪枝)
	+ [5.1 模型剪枝理论](#模型剪枝理论)
	+ [5.2 Tensorflow模型剪枝实践](#Tensorflow模型剪枝实践)
+ [6. 模型量化](#模型量化)
	+ [6.1 模型量化理论](#模型量化理论)
	+ [6.2 Tflite/TensorRT等工具的使用](#Tflite/TensorRT等工具的使用)
+ [7. 模型部署](#模型部署)
	+ [7.1 ONNX的使用](#ONNX的使用)
	+ [7.2 MACE/MNN的使用](#MACE/MNN的使用)
+ [8. 自动化模型设计NAS](#自动化模型设计NAS)
	+ [8.1 NAS理论](#NAS理论)
	+ [8.2 AutoKeras等工具使用](#AutoKeras等工具使用)

***
# 数据使用

## 搜集与整理数据
  很多实际项⽬我们不会有现成的数据集，需要通过额外的⽅法获取和整理。因此项目的第一步就是准备好一个表情识别项⽬所需要的数据集，包括以下几部分：

- 使⽤爬⾍爬取图像和视频，从视频中提取图⽚
- 对获得的图⽚数据进⾏整理，包括重命名，格式统⼀，去重。
- 利⽤⼈脸检测算法删选出有⽤的样本，利⽤关键点检测算法裁剪出嘴唇区域。

(1)数据爬取
	
由于没有直接对应的开源数据集，或者开源数据集中的数据⽐较少，尤其是对于嘟嘴，⼤笑等类的数据，⽽搜索引擎上有海量数据，所以我们可以从中爬取。

*** 

网页由三部分组成，分别是`HTML`、`CSS`和`JavaScript`:
- HTML:超文本链接标示语言。它不是一种编程语言，而是一种标记语言。在HTML中，通常不同类型的文字通过不同类型的标签来表示。如图片用img标签表示，视频用video标签表示，段落用p标签表示。获取一个网页的源代码只需打开网页后按下F12键。
- CSS: HTML仅定义了网页的架构，CSS（层叠样式表）在HTML中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式”指网页中文字大小、颜色、元素间距、排列等格式。
- JavaScript：HTML和CSS只能展现一种静态信息，缺乏交互性。但我们在网页里通常会看到一些交互和动画效果，如提示框、轮播图等，这些动态信息通常就是通过JavaScript完成的。

爬虫最主要的处理对象是**URL**,爬虫的实质就是根据URL地址取得所需要的文件内容，然后对它进行进一步的处理。URL也有它特定的格式，其格式由三部分组成，如下：
- 1. 协议（或称为服务方式）
- 2. 存有该资源的主机IP地址(有时也包括端口号)
- 3. 主机资源的具体地址，如目录和文件名等

> 通常第一部分和第二部分用'://'符号隔开，第二部分和第三部分用'/'符号隔开。另外第一部分和第二部分是不可缺少的，第三部分有时可以省略。 

举例：

http://china.nba.com/lakers/

http这个是协议，也就是HTTP超文本传输协议，它是URL的第一部分；china.nba.com这个是网站名，由服务器名和域名构成，它是URL的第二部分；lakers就是存放网页的根目录，是URL的第三部分。

***

urllib是python自带的一个主要用来爬虫的标准库，无需安装可以直接用，它能完成如下任务：网页请求、响应获取、代理和cookie设置、异常处理和URL解析，可以说要想学会爬虫，必须要学会它。

首先理解什么是GET？什么是POST？它们的区别又是啥？

GET和POST实际上就是HTTP请求的两种基本方法，通常GET是从指定的资源请求数据，而POST是向指定的资源提交要被处理的数据。

已经知道HTTP是基于TCP/IP的关于数据如何在万维网中如何通信的协议。HTTP的底层是TCP/IP，所以GET和POST的底层也是TCP/IP，也就是说，GET/POST都是TCP连接，GET和POST能做的事情是一样的。

那它们的区别体现在哪呢？对于GET方式的请求，浏览器会把http header和data一并发送出去；而对于POST，浏览器先发送header，服务器响应后，浏览器再发送data。

也就是说，在大万维网世界中，TCP就像汽车，我们用TCP来运输数据，HTTP给汽车运输设定了好几个运输方式，有GET, POST等。GET只需要汽车跑一趟就把货送到了，而POST得跑两趟，第一趟，先去和服务器打个招呼“嗨，我等下要送一批货来，你们打开门迎接我”，然后再回头把货送过去。

在urllib中有个request这个模块，它主要是来负责构造和发起网络请求。它有个`urlopen()`访问方法，默认的访问方法是GET，我们在urlopen()方法中传入字符串格式的url地址后，此方法会访问目标网址，然后返回访问的结果。请看下面的实例：

```python
from urllib import request
url = "https://zhuanlan.zhihu.com/p/20751612"
html = request.urlopen(url).read().decode("utf-8")
print(html)
```

`urlopen()`方法请求返回的对象类型为`HTTPResponse`对象。

## 分析数据质量

## 选择好数据的尺度

## 图像分类基本任务中的数据增强实战

***

# 模型使用和调参

## 如何针对自己的任务选择好基准模型架构

## 如何设计与改进模型

## 如何对模型进行训练调参

***

# 模型分析

## 如何对模型进行可视化

## 如何对模型的计算量和参数量进行分析

***

# 紧凑模型设计

## 如何压缩大模型

## 如何设计小模型

## 如何保障小模型的性能

***

# 模型剪枝

## 模型剪枝理论

## Tensorflow模型剪枝实践

***

# 模型量化

## 模型量化理论

## Tflite/TensorRT等工具的使用

*** 

# 模型部署

## ONNX的使用

## MACE/MNN的使用

***

# 自动化模型设计NAS

## NAS理论

## AutoKeras等工具使用
